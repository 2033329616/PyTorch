{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 循环神经网络模块\n",
    "RNN无法有效应对长时依赖问题，时间跨度较大，所以会丢失较前的信息，LSTM (Long Short Term Memory Networks)和GRU (Gated Recurrent Unit)可以一定程度上解决长时依赖问题，但后来提出的注意力attention机制会更加有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 原始RNN\n",
    "网络有两个输入：当前时刻$t$的输入$x_t$和$t-1$时刻的隐藏状态$h_{t-1}$\n",
    "- $x_t$的维度：**(seq, batch, feature)**，分别表示序列长度、批量和输入特征维度\n",
    "- $h_{t-1}$的维度：**(layers*direction, batch, hidden)**，分别表示层数乘方向(单向为1，双向为2)，批量和输出维度\n",
    "\n",
    "网络有两个输出：当前时刻$t$的输出$output$和隐藏状态$h_t$\n",
    "- $output$的维度：**(seq, batch, hidden*direction)**，分别表示序列长度，批量和输出维度乘方向(单向为1，双向为2)\n",
    "- $h_t$的维度：**(layers*direction, batch, hidden)**，分别表示层数乘方向(单向为1，双向为2)，批量和输出维度\n",
    "\n",
    "**注意**：$h_{t-1}$和 $h_t$的维度是一致的；在网络初始化时有隐藏状态$h_0$，可以自己指定，如果不指定则默认为0；在单向的RNN中每层只有一个记忆单元，双向的有两个\n",
    "\n",
    "RNN的内部网络计算公式：\n",
    "$$h_t=tanh(w_{ih}*x_t+b_{ih}+w_{hh}*h_{t-1}+b_{hh})$$\n",
    "\n",
    "在PyTorch中使用`nn.RNN(*args, **kargs)`定义，根据参数的定义形式使用`nn.RNN(input_size=20, hidden_size=50, num_layers=2)`和`nn.RNN(20, 50, 2)`定义的结果一致，参数列表如下：\n",
    "\n",
    "|参数|功能|\n",
    "|-|-|\n",
    "|input_size|输入$x_t$的特征维度|\n",
    "|hidden_size|输出(隐藏状态)$h_t$的特征维度|\n",
    "|num_layers|网络层数，默认为**1**|\n",
    "|nonlinearity|非线性激活函数，默认tanh，可选relu|\n",
    "|bias|是否使用偏置，默认使用**True**|\n",
    "|batch_first|决定网络输入维度的顺序，默认顺序为<br>**(seq, batch, feature)**,如果设置为True，<br>顺序变为(batch, seq, feature)|\n",
    "|dropout|参数接收0~1的数值，在除最后一层外<br>的其他层加dropout层，默认为**0**|\n",
    "|bidirectional|设置为True表示双向循环神经网络，<br>默认**False**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：0.4版本开始的新规范，Variable和tensor合并后程序有所不同\n",
    "参考：[Variable和Tensor合并后，PyTorch的代码要怎么改](https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/80105285)\n",
    "从0.4版本开始程序框架如下：\n",
    "```python\n",
    "# torch.device object used throughout this script\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = MyRNN().to(device)\n",
    " \n",
    "# train\n",
    "total_loss = 0\n",
    "for input, target in train_loader:\n",
    "     input, target = input.to(device), target.to(device)\n",
    "     hidden = input.new_zeros(*h_shape) \n",
    "     # has the same device & dtype as `input`\n",
    "     ...  # get loss and optimize\n",
    "     total_loss += loss.item() # get Python number from 1-element Tensor\n",
    "\n",
    "# evaluate\n",
    "with torch.no_grad():          # operations inside don't track history\n",
    "     for input, target in test_loader:\n",
    "```\n",
    "\n",
    "其他修改部分：\n",
    "1. 使用`x.detach()`代替Variable中的`x.data`来修改数据，仅**原地**修改可改变原tensor\n",
    "2. 损失函数中使用`loss.item()`代替`loss.data[0]`来获取python数值\n",
    "3. 使用`tensor.type()`输出tensor的类型\n",
    "4. `torch.no_grad()`代替`volatile`标志位\n",
    "5. 可以创建零维向量，如`torch.tensor(2)`，更类似于`numpy.array()`函数\n",
    "```python\n",
    "print(torch.tensor(2))  # 只指定数据\n",
    "print(torch.Tensor(2))  # 既可以指定数据，也可以指定维度来随机数据\n",
    "# tensor(2)\n",
    "# tensor(1.00000e-45 *\n",
    "#      [ 1.4013,  0.0000])\n",
    "```\n",
    "\n",
    "注意：\n",
    "\n",
    "\n",
    "- 训练网络的时候把模型和数据都放到GPU上，测试的时候放到CPU上，因为cuda的变量无法直接转变为numpy的变量，测试会涉及到可视化等分析步骤，用numpy数据更佳\n",
    "- `model.to(device)`的方法不依赖设备更加通用，`model.cpu()`或`model.cuda()`转换模型的计算方式(变量同理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# 优先使用GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight_ih的两个权重: torch.Size([50, 20]) torch.Size([50, 50])\n",
      "  bias_ih的权重: torch.Size([50]) torch.Size([50])\n",
      "2.weight_hh的两个权重: torch.Size([50, 50]) torch.Size([50, 50])\n",
      "  bias_hh的权重: torch.Size([50]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "basic_rnn = nn.RNN(input_size=20, hidden_size=50, num_layers=2)\n",
    "print('1.weight_ih的两个权重:', basic_rnn.weight_ih_l0.shape, basic_rnn.weight_ih_l1.shape)\n",
    "print('  bias_ih的权重:', basic_rnn.bias_ih_l0.shape, basic_rnn.bias_ih_l1.shape)\n",
    "print('2.weight_hh的两个权重:', basic_rnn.weight_hh_l0.shape, basic_rnn.weight_hh_l1.shape)\n",
    "print('  bias_hh的权重:', basic_rnn.bias_hh_l0.shape, basic_rnn.bias_hh_l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.RNN的输出output: torch.Size([100, 32, 50])\n",
      "2.RNN的输出隐层状态: torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "# 随机化初始输入xt和隐藏状态h0\n",
    "toy_input = torch.randn([100, 32, 20])            # [seq, batch, feature] \n",
    "h_0 = torch.randn([2, 32, 50])                    # [layers*direction, batch, hidden_size]\n",
    "toy_output, h_n = basic_rnn(toy_input, h_0)\n",
    "print('1.RNN的输出output:', toy_output.shape)      # [seq, batch, hidden_size*direction] \n",
    "print('2.RNN的输出隐层状态:', h_n.shape)           # [layers*direction, batch, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "- 在一层的RNN中，如果是非双向循环的网络，一般只有一个记忆单元，所以隐藏状态的第一维度都是1，双向的网络为2，不同的层有各自的记忆单元，所以隐藏状态$h_t$的第一维度为：**layers*direction**，双向网络direction为2，普通的为1\n",
    "- 下面程序用于显示进度，以后可能会用，注意print函数中设置`end='\\r'`可以覆盖原输出\n",
    "```python\n",
    "def show_progress():\n",
    "    process = '<' + '.'*25 + '>'\n",
    "    for i in range(1, 26):\n",
    "        time.sleep(0.5)\n",
    "        process= process.replace('.', '=', 1)\n",
    "        print(process, end='\\r')\n",
    "    print('\\nfinished!')\n",
    "# 输出\n",
    "# <=========================>\n",
    "# finished!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 9.]) tensor([5., 9.])\n"
     ]
    }
   ],
   "source": [
    "# pytorch中的detach用法\n",
    "t1 = torch.FloatTensor([1., 2.])\n",
    "v1 = Variable(t1)\n",
    "t2 = torch.FloatTensor([2., 3.])\n",
    "v2 = Variable(t2)\n",
    "v3 = v1 + v2\n",
    "v3_detached = v3.detach()      # 只能原地修改\n",
    "v3_detached.data.add_(t1*2)    # 修改了 v3_detached Variable中 tensor 的值\n",
    "print(v3, v3_detached)         # v3 中tensor 的值也会改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 LSTM与GRU\n",
    "本质上与RNN一样，只是内部计算更复杂，参数更多\n",
    "- LSTM做了4次类似标准RNN的运算，所以参数是标准RNN的4倍，隐藏状态为$h_0$和$c_0$，维度均是**(layers*direction, batch, hidden_size)**，direction：单向为1，双向为2\n",
    "- GRU参数是标准RNN的3倍，隐藏状态只有$h_0$，维度同上\n",
    "- LSTM与GRU的定义方法基本与标准RNN的一样，但无nonlinearity参数，其他可参考标准RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight_ih的两个权重: torch.Size([200, 20]) torch.Size([200, 50])\n",
      "  bias_ih的权重: torch.Size([200]) torch.Size([200])\n",
      "2.weight_hh的两个权重: torch.Size([200, 50]) torch.Size([200, 50])\n",
      "  bias_hh的权重: torch.Size([200]) torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义lstm\n",
    "# 参数维度4翻倍：[output_size*4, input_size]\n",
    "lstm = nn.LSTM(input_size=20, hidden_size=50, num_layers=2)\n",
    "print('1.weight_ih的两个权重:', lstm.weight_ih_l0.shape, lstm.weight_ih_l1.shape)\n",
    "print('  bias_ih的权重:', lstm.bias_ih_l0.shape, lstm.bias_ih_l1.shape)\n",
    "print('2.weight_hh的两个权重:', lstm.weight_hh_l0.shape, lstm.weight_hh_l1.shape)\n",
    "print('  bias_hh的权重:', lstm.bias_hh_l0.shape, lstm.bias_hh_l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 50])\n",
      "torch.Size([2, 32, 50])\n",
      "torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "toy_input = torch.randn([100, 32, 20])            # [seq, batch, feature] \n",
    "lstm_out, (h_n, c_n) = lstm(toy_input)\n",
    "print(lstm_out.shape)        # [seq, batch, hidden*direction]\n",
    "print(h_n.shape)             # [layers*direction, batch, hidden]\n",
    "print(c_n.shape)             # [layers*direction, batch, hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight_ih的两个权重: torch.Size([150, 20]) torch.Size([150, 50])\n",
      "  bias_ih的权重: torch.Size([150]) torch.Size([150])\n",
      "2.weight_hh的两个权重: torch.Size([150, 50]) torch.Size([150, 50])\n",
      "  bias_hh的权重: torch.Size([150]) torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "# 2. 定义GRU\n",
    "gru = nn.GRU(input_size=20, hidden_size=50, num_layers=2)\n",
    "print('1.weight_ih的两个权重:', gru.weight_ih_l0.shape, gru.weight_ih_l1.shape)\n",
    "print('  bias_ih的权重:', gru.bias_ih_l0.shape, gru.bias_ih_l1.shape)\n",
    "print('2.weight_hh的两个权重:', gru.weight_hh_l0.shape, gru.weight_hh_l1.shape)\n",
    "print('  bias_hh的权重:', gru.bias_hh_l0.shape, gru.bias_hh_l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 50])\n",
      "torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "gru_out, h_n = gru(toy_input)\n",
    "print(gru_out.shape)         # [seq, batch, hidden*direction]\n",
    "print(h_n.shape)             # [layers*direction, batch, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- nn.RNN、nn.LSTM和nn.GRU定义的是完整序列的网络，而RNNCell、LSTMCell和GRUCell定义单步的网络，即只是序列中的一步，循环神经网络的一个循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 循环神经网络实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 图像分类\n",
    "这里使用MNIST数据集，该数据集中图像是28*28的，可以将图像看作长为28的序列，序列中每个元素特征维度为28，将图像序列化后可以用RNN来处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.导入数据\n",
    "data_tf =transforms.Compose([\n",
    "    transforms.ToTensor(),                   # 将图像转换为[0~1]的tensor\n",
    "    transforms.Normalize([0.5], [0.5])       # 标准化到[-1,1]\n",
    "])\n",
    "# 导入MNIST数据集\n",
    "train_datasets = datasets.MNIST(root='./data/MNIST', train=True, transform=data_tf, download=False)\n",
    "test_datasets = datasets.MNIST(root='./data/MNIST', train=False, transform=data_tf)\n",
    "\n",
    "# [b,28,28] 可以看作 [batch, seq, feature]的序列化数据\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_datasets, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b67391ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 显示数据\n",
    "def show():\n",
    "    labels = test_datasets.test_labels.numpy()  # 没有经过transforms的数据\n",
    "    data = test_datasets.test_data.numpy()\n",
    "    # np.where获得结果:(array([1,...], dtype=int64), )，所以使用[0][index]获取元素\n",
    "    sample_index = [np.where(labels==i)[0][0] for i in range(10)]\n",
    "    sample_data = data[sample_index]\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample_data[i], cmap='gray')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建序列化图像的生成器，为训练模型提供批次数据(该函数不是必须的)\n",
    "# [b,28,28] -> [b, seq, feature]\n",
    "def generator(batch_size=32, train=True, shuffle=True):\n",
    "    datasets = train_datasets if train else test_datasets\n",
    "    # 先生成批次的数据\n",
    "    loader = DataLoader(datasets, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "    for data in loader:\n",
    "       yield data[0], data[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.定义模型\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_layers, n_classes):\n",
    "        super().__init__() \n",
    "        self.n_layers = n_layers       # RNN层数\n",
    "        self.n_classes = n_classes     # 输出类别数\n",
    "        self.hidden_dim = hidden_dim   # 隐藏状态特征数\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=n_layers, batch_first=True)\n",
    "        # batch_first 使输入为 [batch, seq, input_dim]\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)   # 分类\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)     # 传入网络 get [batch,seq,hidden]\n",
    "        out = lstm_out[:, -1, :]       # 获取序列的最后输出 get [batch,hidden]\n",
    "#         print(out.shape)             # get [batch, hidden]\n",
    "        out = self.classifier(out)     # get [batch, n_classes] \n",
    "#         print(out.shape)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.RNN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义损失函数及优化函数\n",
    "rnn_model = RNN(28,256,2,10)           # 创建模型\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-2)\n",
    "type(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 1\n",
      "Batches 0 loss 2.3053\n",
      "Batches 100 loss 1.0990\n",
      "Batches 200 loss 0.8368\n",
      "Batches 300 loss 0.2619\n",
      "Batches 400 loss 0.3684\n",
      "Batches 500 loss 0.5868\n",
      "Batches 600 loss 0.7592\n",
      "Batches 700 loss 0.3227\n",
      "Batches 800 loss 0.4057\n",
      "Batches 900 loss 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 3.训练模型\n",
    "batch_size = 32      # 定义batch_size\n",
    "epochs = 1           # 定义epoch最大次数\n",
    "max_batches = np.ceil(len(train_datasets) / batch_size)*epochs\n",
    "# print(max_batches)\n",
    "\n",
    "def train(model, batch_size, epochs, save_path='./model'):\n",
    "    num_batches = 0                                    # 记录batches的数目\n",
    "    train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=4) \n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print('-'*20)\n",
    "        print('Epoch {}'.format(epoch+1))\n",
    "        for data in train_loader: \n",
    "            # 将变量及模型放到GPU上\n",
    "            x_train, y_train = data   # x维度 [b,1,28,28]\n",
    "            x_train = x_train[:,0]    # x维度 [b,28,28]\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "#             print(x_train.shape)\n",
    "            # forward + backward + optimizer    \n",
    "            out = model(x_train)                  # 计算输出\n",
    "            loss = criterion(out, y_train)        # 计算损失函数\n",
    "            print_loss = loss.item()\n",
    "            optimizer.zero_grad()                 # 梯度归零\n",
    "            loss.backward()                       # 反向传播\n",
    "            optimizer.step()                      # 更新参数\n",
    "#             print(num_batches)\n",
    "                        \n",
    "            if (num_batches) % 100 == 0:\n",
    "#                 print(type(model))\n",
    "                print('Batches {} loss {:.4f}'.format(num_batches, print_loss))\n",
    "            num_batches += 1\n",
    "            if num_batches == 1000:               # 为了快速验证程序，中断训练\n",
    "                break\n",
    "    # 存储路径        \n",
    "    save_path = os.path.join(save_path, 'rnn_{}.pth'.format(num_batches))   \n",
    "    torch.save(model, save_path)                #存储路径\n",
    "     \n",
    "train(rnn_model, batch_size, epochs)         # 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Test Loss: 0.332930, ACC: 0.906250\n",
      "Epoch: 20 Test Loss: 0.530187, ACC: 0.875000\n",
      "Epoch: 30 Test Loss: 0.382257, ACC: 0.875000\n",
      "Epoch: 40 Test Loss: 0.368515, ACC: 0.875000\n",
      "Epoch: 50 Test Loss: 0.474718, ACC: 0.781250\n",
      "Epoch: 60 Test Loss: 0.434582, ACC: 0.875000\n",
      "Epoch: 70 Test Loss: 0.398774, ACC: 0.875000\n",
      "Epoch: 80 Test Loss: 0.546223, ACC: 0.875000\n",
      "Epoch: 90 Test Loss: 0.303690, ACC: 0.875000\n",
      "Epoch: 100 Test Loss: 0.079886, ACC: 1.000000\n",
      "Epoch: 110 Test Loss: 0.376336, ACC: 0.843750\n",
      "Epoch: 120 Test Loss: 0.515528, ACC: 0.843750\n",
      "Epoch: 130 Test Loss: 0.701372, ACC: 0.750000\n",
      "Epoch: 140 Test Loss: 0.368030, ACC: 0.843750\n",
      "Epoch: 150 Test Loss: 0.069468, ACC: 1.000000\n",
      "Epoch: 160 Test Loss: 0.168973, ACC: 0.906250\n",
      "Epoch: 170 Test Loss: 0.042365, ACC: 1.000000\n",
      "Epoch: 180 Test Loss: 0.660746, ACC: 0.843750\n",
      "Epoch: 190 Test Loss: 0.519290, ACC: 0.875000\n",
      "Epoch: 200 Test Loss: 0.189386, ACC: 0.937500\n",
      "Epoch: 210 Test Loss: 0.319277, ACC: 0.906250\n",
      "Epoch: 220 Test Loss: 0.147183, ACC: 0.968750\n",
      "Epoch: 230 Test Loss: 0.150100, ACC: 0.937500\n",
      "Epoch: 240 Test Loss: 0.220129, ACC: 0.937500\n",
      "Epoch: 250 Test Loss: 0.401125, ACC: 0.875000\n",
      "Epoch: 260 Test Loss: 0.423263, ACC: 0.843750\n",
      "Epoch: 270 Test Loss: 0.309275, ACC: 0.906250\n",
      "Epoch: 280 Test Loss: 0.066944, ACC: 1.000000\n",
      "Epoch: 290 Test Loss: 0.084740, ACC: 1.000000\n",
      "Epoch: 300 Test Loss: 0.232688, ACC: 0.875000\n",
      "Epoch: 310 Test Loss: 0.327496, ACC: 0.875000\n",
      "----------Test Done!----------\n",
      "Epoch: 313 Total_Loss: 0.365389 Total_ACC: 0.887000\n"
     ]
    }
   ],
   "source": [
    "# 5.评价模型的性能\n",
    "def test(model, batch_size=32):\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()      # 将模型转换为测试状态\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    test_epoch = 0 \n",
    "    for data in test_loader:\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        img, label = data      # 获取测试数据\n",
    "        img = img[:,0]\n",
    "    #     print(img.shape)\n",
    "        # 由于测试状态下不需方向传播，所以可以释放缓存\n",
    "        with torch.no_grad():\n",
    "\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out = model(img)     # 计算图像的标签 [img.size(0), 10]\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data* label.size(0)   # 计算该批次的总损失\n",
    "            _, pred = torch.max(out, 1)          # 在一行内按列比大小得[64,1],tenso为[64]\n",
    "        #     print(_, pred.shape)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.data\n",
    "        #     print(eval_acc)    \n",
    "            total_loss += eval_loss    # 累计所有批次数据的损失和准确率\n",
    "            total_acc  += eval_acc\n",
    "            if (test_epoch + 1) % 10 == 0:\n",
    "                batch_loss  = (eval_loss / img.size(0))        # 计算该批次的损失\n",
    "                batch_acc   = (eval_acc.float() / img.size(0)) # 计算该批次的准确率\n",
    "                print('Epoch: {} Test Loss: {:.6f}, ACC: {:.6f}'.format(test_epoch+1, batch_loss, batch_acc))\n",
    "            test_epoch += 1\n",
    "    print('----------Test Done!----------')\n",
    "    print('Epoch: {} Total_Loss: {:.6f} Total_ACC: {:.6f}'.format(\\\n",
    "          test_epoch, total_loss/len(test_datasets), float(total_acc)/len(test_datasets)))\n",
    "save_model = torch.load('./model/rnn_1000.pth')\n",
    "save_model.to(device)\n",
    "test(save_model)         # 评估模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
