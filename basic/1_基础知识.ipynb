{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensor(张量)\n",
    "多维矩阵，可以看作可以在GPU上运行的numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|表达形式|数据类型|\n",
    "|-|-|\n",
    "|torch.Tensor|32位浮点型|\n",
    "|torch.FloatTensor|32位浮点型|\n",
    "|torch.DoubleTensor|64位浮点型|\n",
    "|torch.shortTensor|16位整型|\n",
    "|torch.IntTensor|32位整型|\n",
    "|torch.LongTensor|64位整型|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Tensor a is:\n",
      " tensor([[ 2.,  3.],\n",
      "        [ 4.,  8.],\n",
      "        [ 7.,  9.]])\n",
      "2.a shape is: torch.Size([3, 2]) torch.Size([3, 2])\n",
      "3.a type is: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 创建torch向量\n",
    "a = torch.Tensor([[2, 3], [4, 8], [7, 9]])   # 默认的是torch.FloatTensor\n",
    "print('1.Tensor a is:\\n', a)\n",
    "print('2.a shape is:', a.size(), a.shape)    # 可以使用两个方法输出size\n",
    "print('3.a type is:', type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Tensor b is:\n",
      " tensor([[ 2,  3],\n",
      "        [ 4,  8],\n",
      "        [ 7,  9]])\n",
      "3.a type is: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "b = torch.LongTensor([[2, 3], [4, 8], [7, 9]])\n",
    "print('1.Tensor b is:\\n', b)\n",
    "print('3.a type is:', type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Tensor c is:\n",
      " tensor([[ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建零向量\n",
    "c = torch.zeros([3,2])   # 3*2的零向量\n",
    "print('1.Tensor c is:\\n', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Tensor d is:\n",
      " tensor([[ 0.2971,  0.5337],\n",
      "        [ 0.4952, -0.7407],\n",
      "        [-0.1089, -1.1817]])\n"
     ]
    }
   ],
   "source": [
    "# 创建符合高斯分布的随机数据\n",
    "d = torch.randn([3,2])\n",
    "print('1.Tensor d is:\\n', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Tensor a is:\n",
      " tensor([[ 2.,  3.],\n",
      "        [ 4.,  8.],\n",
      "        [ 7.,  9.]])\n",
      "2.Tensor new a is:\n",
      " tensor([[  2.,  77.],\n",
      "        [  4.,   8.],\n",
      "        [  7.,   9.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[2, 3], [4, 8], [7, 9]]) \n",
    "print('1.Tensor a is:\\n', a)\n",
    "a[0, 1] = 77      # a[0][1] = 77，两种方式都可以进行数组元素的改写\n",
    "print('2.Tensor new a is:\\n', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.convert Tensor to <class 'numpy.ndarray'> is:\n",
      "[[2. 3.]\n",
      " [4. 8.]\n",
      " [7. 9.]]\n",
      "2.from numpy to <class 'torch.Tensor'> is:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6]], dtype=torch.int32)\n",
      "3.<class 'torch.Tensor'> is:\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor和numpy.array之间互相转换\n",
    "a = torch.Tensor([[2, 3], [4, 8], [7, 9]]) \n",
    "# print('0.{} a is:\\n{}'.format(type(a), a))\n",
    "numpy_a = a.numpy()            # 转换为numpy的数组\n",
    "print('1.convert Tensor to {} is:\\n{}'.format(type(numpy_a), numpy_a))\n",
    "numpy_e = np.array([[1,2,3],[4,5,6]])\n",
    "# print('2.{} e is:\\n{}'.format(type(numpy_e), numpy_e))\n",
    "e = torch.from_numpy(numpy_e)   # 转换为pytorch的Tensor\n",
    "print('2.from numpy to {} is:\\n{}'.format(type(e), e))\n",
    "f_torch_e = e.float()           # pytorch的Tensor类型转换为float\n",
    "print('3.{} is:\\n{}'.format(type(f_torch_e), f_torch_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch支持GPU\n",
    "a = torch.Tensor([[2, 3], [4, 8], [7, 9]])\n",
    "if torch.cuda.is_available():\n",
    "    a_cuda = a.cuda()\n",
    "    print(a_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- `torch变量.numpy()` pytorch的tensor转换为numpy的array\n",
    "- `torch.from_numpy(numpy变量)` numpy的array转换为pytorch的tensor\n",
    "- `torch变量.X类型()` pytorch的tensor转换为相应的X类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Variable(变量)\n",
    "Variable与Tensor类似，但Variable会放到计算图中，然后进行前向传播、后向传播和自动求导\n",
    "\n",
    "|属性|作用|\n",
    "|-|-|\n",
    "|data|Variable里的tensor数值|\n",
    "|grad|Variable的反向传播梯度|\n",
    "|grad_fn|得到Variable的操作|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.输出梯度\n",
      "x的梯度: tensor([ 3.])\n",
      "w的梯度: tensor([ 2.])\n",
      "b的梯度: tensor([ 1.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1. 对标量求导\n",
    "from torch.autograd import Variable\n",
    "# 创建变量，求导需要传入requires_grad参数，默认为False\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)   # Tensor变为Variable\n",
    "w = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([4]), requires_grad=True)\n",
    "\n",
    "# 构建计算图\n",
    "y = w * x + b\n",
    "\n",
    "# 计算梯度\n",
    "y.backward()   # 对所有需要梯度的变量求导，标量求导不需要参数\n",
    "\n",
    "# 输出梯度\n",
    "print('1.输出梯度')\n",
    "print('x的梯度:', x.grad)\n",
    "print('w的梯度:', w.grad)\n",
    "print('b的梯度:', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 对矩阵求导\n",
    "x = torch.randn([3,2])\n",
    "x = Variable(x, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
