{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 语言表示与语言模型\n",
    "### 1.1 词嵌入(word embedding)\n",
    "图像分类中使用one-hot编码表示不同的类，但在自然语言中，字典(字表)很大如果使用one-hot编码会造成很大的数据稀疏性，并且该编码无法表达单词的语义相似性。自然语言中存在**语义鸿沟**问题，如“麦克风”和“话筒”，无法从字面看出两个单词其实在表达同样的东西，所以引入了基于神经网络的分布式表示，词嵌入(word embedding)或词向量(word vectors)\n",
    "\n",
    "通过一些训练文本，训练word2vec模型，得到单词的高维度表示，优点如下：\n",
    "1. 词向量的夹角越小，表达的语义更加接近(一定程度上解决了语义鸿沟问题)\n",
    "2. 高维向量中元素不是0或1的形式，数据更加稠密"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# 优先使用GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.测试单词的向量类型: torch.LongTensor\n",
      "2.词嵌入的结果: tensor([-0.8307,  1.1385, -0.9492,  1.3911, -1.4588], grad_fn=<EmbeddingBackward>) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 词嵌入，将one-hot编码的单词变为高维度的向量表示\n",
    "word_to_ix = {'hello':0, 'world':1, 'python':2, 'AI':3}\n",
    "embeds = nn.Embedding(3, 5)     # Embedding(m,n) m:字典，所有单词数目 n:嵌入维度\n",
    "test_idx = torch.tensor(word_to_ix['python'])   # 转换为tensor\n",
    "print('1.测试单词的向量类型:', test_idx.type())\n",
    "test_embed = embeds(test_idx)   # 进行词嵌入\n",
    "print('2.词嵌入的结果:', test_embed, test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- `nn.Embedding(m,n)`是将one-hot编码嵌入到相对较低的维度，用float型数据表示，通常词典长度远大于嵌入的维度，但嵌入的维度仍可以看做是高维度，该语句只是通过线性变换将稀疏的数据转换为稠密的低维数据，所以线性变换的权重对应的参数也要进行参数更新，即**词向量也要进行参数的更新**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 语言建模 (Language Model)\n",
    "(1) **N-Gram模型**\n",
    "\n",
    "一句话$T$由$w_1,w_2,...w_n$组成，通过前面的词推断后面的词，用条件概率将词联系到一起，公式如下:\n",
    "\n",
    "<center>$P(T)=P(w_1)P(w_2|w_1)...P(w_n|w_{n-1}w_{n-2}...w_2w_1)$</center>\n",
    "\n",
    "存在问题：预测一个词需要前面所有的词来计算\n",
    "\n",
    "解决方法：引入马尔科夫假设，某个单词只有前面几个词有关系，并不是前面的所有词，一般可以认为距离接近的词之间的联系比距离较远的词联系紧密，引入马尔科夫假设后的N-Gram模型有:\n",
    "1. 一元模型(unigram model)：单词间是独立的概率\n",
    "2. 二元模型(bigram model)：前一个个单词推断后一个单词\n",
    "3. 三元模型(trigram model)：前两个单词推断后一个单词\n",
    "\n",
    "(2) **词袋模型 (Continuous Bag-of-words model,CBOW)**\n",
    "\n",
    "一句话中的第$n$个单词，可以由它前面几个和后面几个单词推断出来，CBOW是通过上下文来预测中间目标词的模型，该模型可以看作N-Gram模型的加强版\n",
    "\n",
    "<img src=\"./image/cbow.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "(3) **Skip-Gram model (SG)**\n",
    "\n",
    "与CBOW相反，是通过中间词来预测上下文\n",
    "<img src=\"./image/skip-gram.jpg\" width=\"50%\" height=\"50%\">\n",
    "注意：语言模型可以通过分层softmax(hierarchical softmax)和负采样(negative sampling)来优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.字典的长度: 622\n",
      "2.训练文本的长度: 1421\n"
     ]
    }
   ],
   "source": [
    "# 1.导入news数据，从China Daily上找的3篇\n",
    "import re  # 导入正则匹配来切分字符串\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"file_path: 读入文本数据的文件路径\n",
    "       vocabulary：返回训练文本的字典\n",
    "       file_idx：返回训练文本对应的字典序号\n",
    "    \"\"\"\n",
    "    assert os.path.exists(file_path), 'file is not exist!'\n",
    "    with open(file_path) as file:\n",
    "        file_str = file.read().strip()   # 读入为字符格式\n",
    "        split_rule = r'[\\s\\.\\,]+'        # 匹配至少一个空格、逗号或句号\n",
    "        file_list = re.split(split_rule, file_str) # 按规则分割文本字符串->list\n",
    "        file_list.remove('\"')            # 移除多余字符\n",
    "        file_set = set(file_list)        # 单词去重\n",
    "        # 创建字典 word:index\n",
    "        vocabulary = {word:i for i, word in enumerate(file_set)}  \n",
    "        file_idx = [vocabulary[word] for word in file_list]\n",
    "#         print(vocabulary)\n",
    "#         print(file_list)\n",
    "#         print(file_idx)\n",
    "        return vocabulary, file_idx\n",
    "\n",
    "vocab, data = read_file('./data/news.txt')\n",
    "print('1.字典的长度:', len(vocab))            # 将所有的词都作为字典了\n",
    "print('2.训练文本的长度:', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一下数据生成器程序参考:[stikbuf/Language_Modeling](https://github.com/stikbuf/Language_Modeling/blob/master/Keras_Character_Aware_Neural_Language_Models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.创建N-Gram的数据集 x(n-i),...,x(n-2),x(n-1)=>x(n)，前i个单词推断当前单词\n",
    "# 数据生成的思想，先将[0,1,...,len(datasets)-1]索引随机，选出batch_size个索引\n",
    "# 通过该索引使用datasets[i:i+num_infer+1]读取所需数据，但注意索引溢出\n",
    "def create_data(datasets, num_infer, batch_size):\n",
    "    \"\"\"datasets:数据集list形式\n",
    "       num_infer:需要参考的词数量\n",
    "       X:所需的推测数据 [batch, num_infer] 列表形式\n",
    "       y:目标数据 [batch, 1]\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        rnd_idx = list(range(len(datasets) - num_infer - 1)) # 保证索引不溢出\n",
    "        np.random.shuffle(rnd_idx)    # 打乱顺序，随机取出一个batch的数据\n",
    "        batch_start = 0               # 按batch_size大小读取数据的索引\n",
    "        X, Y = [], []\n",
    "        while batch_start + batch_size < len(rnd_idx):    # 保证索引不溢出\n",
    "            # 取batch_size个索引\n",
    "            batch_idx = rnd_idx[batch_start:batch_start+batch_size] \n",
    "            temp_data = np.array([datasets[i:i+num_infer+1] for i in batch_idx])\n",
    "#             print(temp_data)\n",
    "\n",
    "            X = temp_data[:, :-1]       # 除去最后一列的所有数据\n",
    "            Y = temp_data[:, -1:]       # 最后一列数据 \n",
    "            batch_start += batch_size   # 方便读取下一个batch索引\n",
    "            yield X, Y\n",
    "        \n",
    "gen = create_data(data, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.训练数据X:\n",
      " [[ 60 135]\n",
      " [195  18]\n",
      " [110 197]]\n",
      "2.目标数据y:\n",
      " [[317]\n",
      " [  1]\n",
      " [223]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(gen)\n",
    "print('1.训练数据X:\\n', x)\n",
    "print('2.目标数据y:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.创建模型\n",
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, n_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_word = vocab_size                            # 训练数据的字典大小\n",
    "        self.context_size = context_size\n",
    "        self.n_dim = n_dim\n",
    "        # 嵌入后的维度 [batch, context_size] -> [batch, context_size, n_dim]\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)   # 嵌入维度为n_dim\n",
    "        # get [batch, hidden_dim]\n",
    "        self.linear1 = nn.Linear(context_size*n_dim, hidden_dim)\n",
    "        # get [batch, self.n_word] 与字典索引对应\n",
    "        self.linear2 = nn.Linear(hidden_dim, self.n_word)          # 输出字典的维度\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "         # get [batch,context_size*n_dim]\n",
    "        embeds = embeds.view(-1, self.context_size*self.n_dim)\n",
    "        out = self.linear1(embeds)        # get [batch, hidden_dim]\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)           # get [batch, n_word]\n",
    "        log_prob = F.log_softmax(out, 1)  # 沿第dim=1轴计算\n",
    "        return log_prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.输入的数据类型: torch.LongTensor\n",
      "2.模型输出: torch.Size([5, 1421]) \n",
      " tensor([[-7.2300, -7.2635, -7.2820,  ..., -7.1418, -6.7750, -6.9280],\n",
      "        [-7.3038, -6.7770, -7.3856,  ..., -7.4613, -7.2551, -7.3360],\n",
      "        [-7.4271, -6.9688, -7.2909,  ..., -7.1448, -7.0421, -7.1410],\n",
      "        [-7.0822, -6.5178, -7.6768,  ..., -7.3704, -6.9111, -7.0851],\n",
      "        [-7.3081, -6.6761, -7.3068,  ..., -7.4454, -7.2807, -6.7935]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 创建模型并进行测试\n",
    "context = 2  # 参考前2各词\n",
    "inputs = np.random.randint(0,len(data), [5,2])\n",
    "inputs = torch.from_numpy(inputs).long()   # 必须将数据转换为LongTensor才行\n",
    "print('1.输入的数据类型:', inputs.type())\n",
    "NGram_model = NgramModel(len(data), context, 128, hidden_dim=256)\n",
    "outputs = NGram_model(inputs)\n",
    "print('2.模型输出:', outputs.shape, '\\n', outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.定义损失函数，优化函数\n",
    "criterion = nn.CrossEntropyLoss()  # 网络输出vocab维度，目标vocab维度，类似于分类\n",
    "optimizer = optim.SGD(NGram_model.parameters(), lr=1e-2)#, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 59, 226],\n",
       "        [251, 514],\n",
       "        [141,  59]]), array([[167],\n",
       "        [446],\n",
       "        [339]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.0934\n",
      "epoch 2000 loss 0.1688\n",
      "epoch 4000 loss 0.1514\n",
      "epoch 6000 loss 0.1482\n",
      "epoch 8000 loss 0.2233\n",
      "epoch 10000 loss 0.1112\n",
      "epoch 12000 loss 0.1064\n",
      "epoch 14000 loss 0.0548\n",
      "epoch 16000 loss 0.1644\n",
      "epoch 18000 loss 0.1428\n",
      "epoch 20000 loss 0.1721\n",
      "epoch 22000 loss 0.2054\n",
      "epoch 24000 loss 0.2478\n",
      "epoch 26000 loss 0.1504\n",
      "epoch 28000 loss 0.1263\n",
      "epoch 30000 loss 0.1749\n",
      "epoch 32000 loss 0.1962\n",
      "epoch 34000 loss 0.3274\n",
      "epoch 36000 loss 0.2761\n",
      "epoch 38000 loss 0.1430\n",
      "epoch 40000 loss 0.2732\n",
      "epoch 42000 loss 0.2349\n",
      "epoch 44000 loss 0.2268\n",
      "epoch 46000 loss 0.2193\n",
      "epoch 48000 loss 0.1436\n"
     ]
    }
   ],
   "source": [
    "# 5.训练网络\n",
    "context = 2              # 参考的单词数目\n",
    "epochs = 50000\n",
    "batch_size = 32\n",
    "def train(model, batch_size, epochs):\n",
    "    model.to(device)     # 优先使用GPU\n",
    "    gen = create_data(data, context, batch_size)  # 创建数据生成器\n",
    "    train_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        x, y = next(gen)  # 获取训练数据\n",
    "        x, y = map(lambda x:torch.LongTensor(x).to(device), [x, y])\n",
    "        y = y.squeeze()\n",
    "        prediction = model(x)\n",
    "#         print(prediction.shape)\n",
    "#         print(y.shape)\n",
    "        loss = criterion(prediction, y)\n",
    "#         train_loss += loss.item()    # 叠加\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 2000 == 0:\n",
    "            print('epoch {} loss {:.4f}'.format(epoch, loss.item()))\n",
    "        \n",
    "            \n",
    "train(NGram_model, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "1.测试的输入单词 ['year', 'earlier']  输出单词 ['to']\n",
      "2.预测的单词为  to\n",
      "------------------------------------------------------------\n",
      "1.测试的输入单词 ['often', 'sees']  输出单词 ['Chinese']\n",
      "2.预测的单词为  Chinese\n",
      "------------------------------------------------------------\n",
      "1.测试的输入单词 ['welcomed', 'the']  输出单词 ['company']\n",
      "2.预测的单词为  company\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = create_data(data, context, batch_size=1)\n",
    "idx_to_word = {idx:word for word, idx in vocab.items()}   # 创建index:word的字典\n",
    "\n",
    "def test(num_samples=1):\n",
    "    print('-'*60)\n",
    "    for i in range(num_samples):\n",
    "        word, label = next(gen)\n",
    "        # print(word)\n",
    "        # print(label)\n",
    "        # [[220 483]]获取数据到list\n",
    "        inputs_idx = [word[0,i] for i in range(word.shape[1])] \n",
    "        # print(inputs_idx)\n",
    "\n",
    "        print('1.测试的输入单词 {}'.format([idx_to_word[idx] for idx in inputs_idx]), end='')\n",
    "        print('  输出单词 {}'.format([idx_to_word[label[0,0]]]))\n",
    "\n",
    "        word = torch.from_numpy(word).long().to(device)\n",
    "        prediction = NGram_model(word)\n",
    "        pred_label_idx = prediction.max(1)[1].item()\n",
    "        # print(pred_label_idx)\n",
    "        print('2.预测的单词为 ', idx_to_word[pred_label_idx])\n",
    "        print('-'*60)\n",
    "test(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- `CrossEntropyLoss()`在分类任务中标签非one-hot编码下使用，输出维度[batch, out_dim]，目标维度[batch]仅是一维的数组，且里面的元素属于[0,num_classes-1]之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
