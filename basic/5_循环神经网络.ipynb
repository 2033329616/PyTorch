{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 循环神经网络模块\n",
    "RNN无法有效应对长时依赖问题，时间跨度较大，所以会丢失较前的信息，LSTM (Long Short Term Memory Networks)和GRU (Gated Recurrent Unit)可以一定程度上解决长时依赖问题，但后来提出的注意力attention机制会更加有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 原始RNN\n",
    "网络有两个输入：当前时刻$t$的输入$x_t$和$t-1$时刻的隐藏状态$h_{t-1}$\n",
    "- $x_t$的维度：`(seq, batch, feature)`，分别表示序列长度、批量和输入特征维度\n",
    "- $h_{t-1}$的维度：`(layers*direction, batch, hidden)`，分别表示层数乘方向(单向为1，双向为2)，批量和输出维度\n",
    "\n",
    "网络有两个输出：当前时刻$t$的输出$output$和隐藏状态$h_t$\n",
    "- $output$的维度：`(seq, batch, hidden*direction)`，分别表示序列长度，批量和输出维度乘方向(单向为1，双向为2)\n",
    "- $h_t$的维度：`(layers*direction, batch, hidden)`，分别表示层数乘方向(单向为1，双向为2)，批量和输出维度\n",
    "\n",
    "**注意**：$h_{t-1}$和 $h_t$的维度是一致的；在网络初始化时有隐藏状态$h_0$，可以自己指定，如果不指定则默认为0；在单向的RNN中每层只有一个记忆单元，双向的有两个\n",
    "\n",
    "RNN的内部网络计算公式：\n",
    "$$h_t=tanh(w_{ih}*x_t+b_{ih}+w_{hh}*h_{t-1}+b_{hh})$$\n",
    "\n",
    "在PyTorch中使用`nn.RNN(*args, **kargs)`定义，由于参数的形式使用`nn.RNN(input_size=20, hidden_size=50, num_layers=2)`和`nn.RNN(20, 50, 2)`定义的结果一致，参数列表如下：\n",
    "\n",
    "|参数|功能|\n",
    "|-|-|\n",
    "|input_size|输入$x_t$的特征维度|\n",
    "|hidden_size|输出(隐藏状态)$h_t$的特征维度|\n",
    "|num_layers|网络层数，默认为**1**|\n",
    "|nonlinearity|非线性激活函数，默认tanh，可选relu|\n",
    "|bias|是否使用偏置，默认使用**True**|\n",
    "|batch_first|决定网络输入维度的顺序，默认顺序为<br>**(seq, batch, feature)**,如果设置为True，<br>顺序变为(batch, seq, feature)|\n",
    "|dropout|参数接收0~1的数值，在除最后一层外<br>的其他层加dropout层，默认为**0**|\n",
    "|bidirectional|设置为True表示双向循环神经网络，<br>默认**False**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：从该篇起使用0.4版本以来的新规范，Variable和tensor合并后程序有所不同\n",
    "参考：[Variable和Tensor合并后，PyTorch的代码要怎么改](https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/80105285)\n",
    "从0.4版本开始程序框架如下：\n",
    "```python\n",
    "# torch.device object used throughout this script\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = MyRNN().to(device)\n",
    " \n",
    "# train\n",
    "total_loss = 0\n",
    "for input, target in train_loader:\n",
    "     input, target = input.to(device), target.to(device)\n",
    "     hidden = input.new_zeros(*h_shape)  # has the same device & dtype as `input`\n",
    "     ...  # get loss and optimize\n",
    "     total_loss += loss.item()           # get Python number from 1-element Tensor\n",
    "\n",
    "# evaluate\n",
    "with torch.no_grad():                   # operations inside don't track history\n",
    "     for input, target in test_loader:\n",
    "```\n",
    "\n",
    "其他修改部分：\n",
    "1. 使用`x.detach()`代替Variable中的`x.data`来修改数据\n",
    "2. 损失函数中使用`loss.item()`代替`loss.data[0]`来获取python数值\n",
    "3. 使用`tensor.type()`输出tensor的类型\n",
    "4. `torch.no_grad()`代替`volatile`标志位\n",
    "5. 可以创建零维向量，如`torch.tensor(2)`，更类似于`numpy.array()`函数\n",
    "```python\n",
    "print(torch.tensor(2))  # 只指定数据\n",
    "print(torch.Tensor(2))  # 既可以指定数据，也可以指定维度来随机数据\n",
    "# tensor(2)\n",
    "# tensor(1.00000e-45 *\n",
    "#      [ 1.4013,  0.0000])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight_ih的两个权重: torch.Size([50, 20]) torch.Size([50, 50])\n",
      "  bias_ih的权重: torch.Size([50]) torch.Size([50])\n",
      "2.weight_hh的两个权重: torch.Size([50, 50]) torch.Size([50, 50])\n",
      "  bias_hh的权重: torch.Size([50]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "basic_rnn = nn.RNN(input_size=20, hidden_size=50, num_layers=2)\n",
    "print('1.weight_ih的两个权重:', basic_rnn.weight_ih_l0.shape, basic_rnn.weight_ih_l1.shape)\n",
    "print('  bias_ih的权重:', basic_rnn.bias_ih_l0.shape, basic_rnn.bias_ih_l1.shape)\n",
    "print('2.weight_hh的两个权重:', basic_rnn.weight_hh_l0.shape, basic_rnn.weight_hh_l1.shape)\n",
    "print('  bias_hh的权重:', basic_rnn.bias_hh_l0.shape, basic_rnn.bias_hh_l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.RNN的输出output: torch.Size([100, 32, 50])\n",
      "2.RNN的输出隐层状态: torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "# 随机化初始输入xt和隐藏状态h0\n",
    "toy_input = torch.randn([100, 32, 20])            # [seq, batch, feature] \n",
    "h_0 = torch.randn([2, 32, 50])                    # [layers*direction, batch, hidden_size]\n",
    "toy_output, h_n = basic_rnn(toy_input, h_0)\n",
    "print('1.RNN的输出output:', toy_output.shape)      # [seq, batch, hidden_size*direction] \n",
    "print('2.RNN的输出隐层状态:', h_n.shape)           # [layers*direction, batch, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "- 在一层的RNN中，如果是非双向循环的网络，一般只有一个记忆单元，所以隐藏状态的第一维度都是1，双向的网络为2，不同的层有各自的记忆单元，所以隐藏状态$h_t$的第一维度为：**layers*direction**，双向网络direction为2，普通的为1\n",
    "- 下面程序用于显示进度，以后可能会用，注意print函数中设置`end='\\r'`可以覆盖原输出\n",
    "```python\n",
    "def show_progress():\n",
    "    process = '<' + '.'*25 + '>'\n",
    "    for i in range(1, 26):\n",
    "        time.sleep(0.5)\n",
    "        process= process.replace('.', '=', 1)\n",
    "        print(process, end='\\r')\n",
    "    print('\\nfinished!')\n",
    "# 输出\n",
    "# <=========================>\n",
    "# finished!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.Tensor([[1,2],[3,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
