{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类器(torchtext处理数据)\n",
    "参考：[Torchtext 详细介绍 Part.1](https://zhuanlan.zhihu.com/p/37223078) , [github](https://github.com/keitakurita/practical-torchtext/blob/master/Lesson%201%20intro%20to%20torchtext%20with%20text%20classification.ipynb)\n",
    ", [手把手教你用torchtext处理文本数据](https://cloud.tencent.com/developer/article/1168890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join('../..', 'data','text')         # 设置数据的根目录\n",
    "data_list = ['train', 'val', 'test']                     # 设置train,val,test路径\n",
    "train_csv, val_csv, test_csv = map(lambda x:os.path.join(data_root, x+'.csv'), data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据如下，“评论文本”与其对应的标签“恶意，严重恶意，淫秽，威胁，侮辱和身份仇恨”，该数据集可以训练一个**文本分类器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_csv).head(2)                          # 显示训练集数据 \n",
    "# pd.read_csv(val_csv).head(2)                          # 验证集与训练集的数据类型一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(test_csv).head(2)                           # 显示测试集数据 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  声明Field对象定义数据预处理的pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 定义tokenizer函数\n",
    "进行数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en')               # 导入英语模型\n",
    "MAX_CHARS = 20000\n",
    "def tokenizer(comment):         \n",
    "    \"\"\"\n",
    "        func: 数据清洗及预处理，并返回token标记(字母与数字)\n",
    "        comment：传入需要处理的文本字符串(str)\n",
    "    \"\"\"\n",
    "    comment = re.sub(r'[\\*\\\"“”\\n\\.\\+\\-\\/\\=\\(\\)\\!;\\\\]', \" \", str(comment))   # 滤除无用的字符串\n",
    "    comment = re.sub(r'\\s+', ' ', comment)               # 将多个空格合并为一个空格\n",
    "    comment = re.sub(r'\\!+', '!', comment)               # 将多个 ‘!’ 合并为一个\n",
    "    comment = re.sub(r'\\,+', ',', comment)               # 同上\n",
    "    comment = re.sub(r'\\?+', '?', comment)               # 同上\n",
    "    \n",
    "    if len(comment) > MAX_CHARS:                         # 如果数据过长就截断\n",
    "        comment = comment[:MAX_CHARS] \n",
    "    # 仅返回字符和数字，滤除标点符号\n",
    "    return [token.text for token in NLP.tokenizer(comment) if not token.is_space  and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'hahah', 'abc', 'def', 'class', 'printf', 'sub', 'fdf', '233']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试tokenizer函数\n",
    "test_doc = 'abc .... hahah,,, ..?? abc def class “ ” printf sub!! @  :  (\\) fdf / 233((()))'\n",
    "test_token = tokenizer(test_doc)\n",
    "test_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义Field对象\n",
    "声明数据预处理的pipline\n",
    "\n",
    "如果数据是文本段落，需设置`sequential=True`及`use_vocab=True`，如果处理的数据已经是数字，那将这两个标志设为False即可，`batch_first=True`将batch放在第一维，sequence放到第二维\n",
    "\n",
    "更多参数参考：[官方注释](https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61) 或 [blog：Torchtext指南 （侧重于NMT）](http://www.cnblogs.com/helloeboy/p/9882467.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, lower=True, use_vocab=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建数据集\n",
    "### 3.1数据预处理\n",
    "根据Field的声明来处理数据\n",
    "csv数据一共8列，数据处理时必须按照列的顺序，传入`[(name, field), ]`列表作为fields的参数，没有使用的列使用`None`来声明，如`('id', None)`，其他的列按照定义好的Field类来处理相应的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train_val_fields = [('id', None),       ('comment_text', TEXT),\n",
    "                    ('toxic', LABEL),   ('severe_toxic', LABEL),\n",
    "                    ('obscene', LABEL), ('threat', LABEL),\n",
    "                    ('insult', LABEL),   ('identity_hate', LABEL)]\n",
    "train_dataset, val_dataset = TabularDataset.splits(      # 同时处理多个数据集，用splits\n",
    "    path = data_root,                                    # 数据所在的目录\n",
    "    train = 'train.csv', validation = 'val.csv',\n",
    "    format = 'csv',                                      # 指定处理文件格式\n",
    "    skip_header = True,                                  # 如果csv有表头，则设置该参数，跳过表头\n",
    "    fields = train_val_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据没有标签所以处理方法与训练集和验证集不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_fields = [('id', None), ('comment_text', TEXT)]\n",
    "test_dataset = TabularDataset(                           # 只处理一个数据集\n",
    "    path = os.path.join(data_root, 'test.csv'),\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = test_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集大多和列表一样可以使用**索引**的方式来访问得到某个样本，如`train_dataset[0]`，但每个样本数据都是**字典**形式来存储，使用`sample.comment_text`来访问，其中comment_text是数据集某一列的**表头**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x25d7ff4cc88>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]             # 取出第一个数据\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.__dict__.keys()                # 第一个数据以字典的形式存储 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.文本数据： ['explanation', 'why', 'the', 'edits', 'made', 'under', 'my']\n",
      "2.标签数据： 0\n"
     ]
    }
   ],
   "source": [
    "print('1.文本数据：', sample.comment_text[:7])       # 每个样本可以使用字典的形式来访问\n",
    "print('2.标签数据：',sample.toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 根据训练集来建立词典\n",
    "`stoi`属性返回{word:index,...}形式的**字典**(collections.defaultdict)，`itos`属性返回[word,...]形式的**列表**\n",
    "可以使用`max_size`和`min_freq`来表示词汇表(字典)中有多少单词和单词最少频率，为出现在词汇表的单词转换为`<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(train_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 80), ('to', 41), ('you', 38), ('i', 32), ('of', 30)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(5)        # freqs是collections.Counter类型，统计出现频率高的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>torchtext读取数据的几个类</center>\n",
    "\n",
    "|类|描述|使用场景\n",
    "|-|-|-|\n",
    "|TabularDataset          |处理的文件类型有csv/tsv、json及python的dict |每一条文本都有一个(或多个)标签的问题，如文本分类|\n",
    "|LanguageModelingDataset |以text文件的路径为输入                      |语言模型|\n",
    "|TranslationDataset      |以每种语言文件的路径或扩展名为输入，例英语文件是\"hoge.en\"，法语文件是\"hoge,fr\"，输入为path=\"hoge\", exts={\"en\", \"fr\"}|机器翻译|\n",
    "|SequenceTaggingDataset  |输入是由tabs分割的输入序列和输出序列的文件路径|序列标注|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 创建迭代器\n",
    "上述步骤已经将数据读入到内存，在该步骤中将数据划分为批次，方便送入网络\n",
    "\n",
    "设置`sort_within_batch=False`，按照`sort_key`对每个批次内进行降序排列，用于对序列进行padding，将序列变为等长，这里的`sort_key = lambda token:len(token.comment_text)`语句表示根据单词长度来进行padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter, val_iter = BucketIterator.splits((train_dataset, val_dataset),  # 传入数据集\n",
    "        batch_sizes = (25,25),                     # (train_batch, val_batch)\n",
    "        device = -1,                               # CPU:-1 GPU:指定GPU编号，从0开始\n",
    "        sort_key = lambda token:len(token.comment_text),   # 依据文本的长度对数据进行分组\n",
    "        sort_within_batch = False,\n",
    "        repeat = False)                            # 设置为False，可以对迭代器进行再次包装\n",
    "                                            \n",
    "test_iter = Iterator(test_dataset, batch_size=25, device=-1,\n",
    "                    sort=False, sort_within_batch=False, repeat=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 25]\n",
       "\t[.comment_text]:[torch.LongTensor of size 25x503]\n",
       "\t[.toxic]:[torch.LongTensor of size 25]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 25]\n",
       "\t[.obscene]:[torch.LongTensor of size 25]\n",
       "\t[.threat]:[torch.LongTensor of size 25]\n",
       "\t[.insult]:[torch.LongTensor of size 25]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 25]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取批次化的数据\n",
    "sample_batch = next(iter(train_iter))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'train', 'fields', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[600,  12,   2,  ...,   1,   1,   1],\n",
       "        [101,   3, 172,  ...,   1,   1,   1],\n",
       "        [200,  48, 198,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [368,  35, 495,  ...,   1,   1,   1],\n",
       "        [  2, 205, 254,  ...,   1,   1,   1],\n",
       "        [313,  15,   7,  ...,   1,   1,   1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch.comment_text         # 文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch.toxic               # 标签数据 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意这种自定义数据类型使代码重用很难，当csv数据的列名(表头)改变时，代码也要改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 包装迭代器\n",
    "在上述数据集中有多个标签，而迭代器返回一个文本向量和多个标签向量，但训练模型时标签一般都是一个向量，所以需要将上述迭代器产生的结果进行包装，最终得到(x,y)形式的样本和标签对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper(object):\n",
    "    def __init__(self, data_iter, x_var, y_vars):\n",
    "        \"\"\"\n",
    "        func: 对数据迭代器进行包装，返回(x,y)形式的批次形式\n",
    "        data_iter: 数据迭代器\n",
    "        x_var: 作为数据x的变量名称(列表)\n",
    "        y_vars：作为标签y的变量名称(列表)\n",
    "        \"\"\"\n",
    "        self.data_iter, self.x_var, self.y_vars = data_iter, x_var, y_vars\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_iter:\n",
    "            x = getattr(batch, self.x_var)           # 提取数据的x_var属性，这里对应comment_text\n",
    "            \n",
    "            if self.y_vars is not None:              # 把y拼接为一个向量\n",
    "                # (N,) => (N,1) 再进行拼接\n",
    "                temp = [getattr(batch, attr).unsqueeze(1) for attr in self.y_vars]\n",
    "                y = torch.cat(temp, dim=1).float()   # 并转换类型，标签需要float类型\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield (x, y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = 'comment_text'\n",
    "y_vars = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_loader = BatchWrapper(train_iter, x_var, y_vars)\n",
    "val_loader   = BatchWrapper(val_iter, x_var, y_vars)\n",
    "test_loader  = BatchWrapper(test_iter, x_var, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.x的tensor: torch.Size([25, 503])\n",
      "2.y的tensor: torch.Size([25, 6])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(train_loader.__iter__())\n",
    "print('1.x的tensor:', x_batch.shape)\n",
    "print('2.y的tensor:', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练文本分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 定义文本分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_linear=0):\n",
    "        \"\"\"\n",
    "        func: 定义简单的LSTM模型进行文本分类\n",
    "        embed_size：嵌入的维度大小\n",
    "        hidden_size：lstm隐藏状态的大小\n",
    "        num_linear：全连接的层数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 词汇表的大小是len(TEXT.vocab)\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), embed_size)      # (N,seq,embed_size)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.linear_layers = []                       # 中间全连接层的列表 \n",
    "        # 将中间层添加到模型中\n",
    "        for _ in range(num_linear):\n",
    "            self.linear_layers.append(nn.Linear(hidden_size, hidden_size)) \n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)         # 转换为模型层\n",
    "        # 输出层\n",
    "        self.predictor = nn.Linear(hidden_size, 6)     # 6个标签，所以输出维度为6\n",
    "   \n",
    "    def forward(self, seq):\n",
    "        embedded_out = self.embedding(seq)             # (N,seq) => (N,seq,embed_size)\n",
    "        lstm_out, lsmt_hidden = self.encoder(embedded_out)     # (N,seq,hidden)\n",
    "        feature = lstm_out[:,-1,:]                     # 取最后step的输出 (N,hidden) \n",
    "        for layer in self.linear_layers:               # (N,hidden)\n",
    "            feature = layer(feature)\n",
    "        predicts = self.predictor(feature)             # (N,6) 未经过log_softmax处理\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 训练文本分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 500\n",
    "lstm_model = LSTMBaseline(embedding_size, hidden_size)      # 定义模型\n",
    "criterion = nn.BCEWithLogitsLoss()                          # 使用二分类损失\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs=1):\n",
    "    \"\"\"\n",
    "    func：训练模型\n",
    "    train_data：训练数据集\n",
    "    val_data：验证集\n",
    "    epochs：训练epochs\n",
    "    \"\"\"\n",
    "    pltLoss = []\n",
    "    running_loss = 0\n",
    "    GPU = lambda x:x.to(device)                        # 优先使用GPU\n",
    "    model.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()                                  # 训练模式\n",
    "        for data in train_loader:\n",
    "            x, y = data\n",
    "            x, y = map(GPU, [x, y])\n",
    "            preds = model(x)\n",
    "            prob = 1 /(1+torch.exp(-preds.data) )\n",
    "\n",
    "            loss = criterion(preds, y)\n",
    "            print_loss = loss.item()                   # 获取loss数据            \n",
    "            optimizer.zero_grad()                      # 更新参数\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += print_loss * x.shape[0]\n",
    "\n",
    "            break\n",
    "        epoch_loss = running_loss / len(train_loader)           # 计算epoch的损失\n",
    "        running_loss = 0                                        # 切记epoch完成后清零\n",
    "        pltLoss.append(epoch_loss)\n",
    "        \n",
    "        # 计算验证集上的误差\n",
    "        val_loss = 0\n",
    "        model.eval()                                            # 模型进入测试模型\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = map(GPU, [x_val, y_val])         # 数据放到GPU上\n",
    "                preds = model(x_val)       \n",
    "                loss = criterion(preds, y_val)\n",
    "                val_loss += loss.item() * x_val.shape[0]\n",
    "            epoch_val_loss = val_loss / len(val_loader)\n",
    "        print('Epoch: {:<3} Train Loss: {:<8.4f} Val Loss: {:<7.4f}'.format(\n",
    "            epoch, epoch_loss, epoch_val_loss))\n",
    "    return model, pltLoss          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train Loss: 17.4492  Val Loss: 14.5220\n",
      "Epoch: 2   Train Loss: 14.4247  Val Loss: 9.6551 \n",
      "Epoch: 3   Train Loss: 9.2733   Val Loss: 6.1040 \n",
      "Epoch: 4   Train Loss: 5.2720   Val Loss: 5.6771 \n",
      "Epoch: 5   Train Loss: 4.4929   Val Loss: 5.6469 \n",
      "Epoch: 6   Train Loss: 3.9501   Val Loss: 5.4768 \n",
      "Epoch: 7   Train Loss: 3.8724   Val Loss: 5.5438 \n",
      "Epoch: 8   Train Loss: 3.8651   Val Loss: 5.5877 \n",
      "Epoch: 9   Train Loss: 3.8549   Val Loss: 5.5679 \n",
      "Epoch: 10  Train Loss: 3.8055   Val Loss: 5.5063 \n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 开始训练模型\n",
    "epochs = 10\n",
    "model, plt_loss= train(lstm_model, train_loader, val_loader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25d4d79e978>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d7ff2f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title('loss curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(plt_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 评估模型性能\n",
    "在上述模型中，相当于进行了6个二分类，而网络输出的是未进行归一化的分数，所以为了得到概率表示，可以使用sigmoid处理分数向量，得到各自的概率，可以把每一类比作一个神经元，所以使用sigmoid来计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "model.eval()\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        preds = model(x)\n",
    "        preds = preds.data.numpy()                  # 将数据转换为numpy格式\n",
    "        probs = 1 / (1 + np.exp(-preds))            # sigmoid函数处理\n",
    "        test_preds.append(probs)\n",
    "test_preds = np.vstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_csv)\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.141822</td>\n",
       "      <td>0.024757</td>\n",
       "      <td>0.028517</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.141822</td>\n",
       "      <td>0.024757</td>\n",
       "      <td>0.028517</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.141822</td>\n",
       "      <td>0.024757</td>\n",
       "      <td>0.028517</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "\n",
       "      toxic  severe_toxic   obscene   threat    insult  identity_hate  \n",
       "0  0.141822      0.024757  0.028517  0.00154  0.014846       0.001408  \n",
       "1  0.141822      0.024757  0.028517  0.00154  0.014846       0.001408  \n",
       "2  0.141822      0.024757  0.028517  0.00154  0.014846       0.001408  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "\n",
    "torchtext处理数据的步骤为：\n",
    "1. 定义Field类，声明数据预处理的方式\n",
    "2. 使用TabularDataset等数据集类按照Field的要求处理数据\n",
    "3. 使用BucketIterator或Iteration迭代器将数据集分成批次数据(此时数据就可以输入到网络了)\n",
    "4. 可以对迭代器再次进行包装，如这里的多标签向量进行合并得到一个表情向量，最终得到`(x,y)`数据和标签对\n",
    "\n",
    "spacy自然语言处理库可以进行分词、实体识别、和依赖关系划分等功能，并且速度很快"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
