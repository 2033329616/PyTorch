{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言建模(Language Modeling)\n",
    "语言建模中我们可以判断一句话是否接近人说的话，一般给出前面的词来预测后面的词，而语言建模的数据集是大量的语料(corpus)，将语料数据进行预处理，结果形式如下：\n",
    "- 输入：`<SOS> Life is short, I use python.`\n",
    "- 输出：`Life is short, I use python. <EOS>`\n",
    "\n",
    "使用这样的文本对来训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32              # 批次大小\n",
    "seq_len = 25                # 句子长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 定义token函数\n",
    "`<`或`>`在spacy中当作非标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "NLP = spacy.load('en')                                          # 载入英语模型\n",
    "NLP.tokenizer.add_special_case('<eos>', [{ORTH: '<eos>'}])      # 终止符，表示文本中的特殊符号\n",
    "NLP.tokenizer.add_special_case('<sos>', [{ORTH: '<sos>'}])      # 起始符\n",
    "NLP.tokenizer.add_special_case('<unk>', [{ORTH: '<unk>'}])      # 用于标记不在词典范围内的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):         \n",
    "    \"\"\"\n",
    "        func: 数据清洗及预处理，并返回token标记(字母与数字)\n",
    "        text：传入需要处理的文本字符串(str)\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[\\*\\\"“”\\n\\.\\+\\-\\/\\=\\(\\)\\!;\\\\]', \" \", str(text))   # 滤除无用的字符串\n",
    "    text = re.sub(r'\\s+', ' ', text)               # 将多个空格合并为一个空格\n",
    "    text = re.sub(r'\\!+', '!', text)               # 将多个 ‘!’ 合并为一个\n",
    "    text = re.sub(r'\\,+', ',', text)               # 同上\n",
    "    text = re.sub(r'\\?+', '?', text)               # 同上\n",
    "    \n",
    "    # 仅返回字符和数字，滤除标点符号\n",
    "    return [token.text for token in NLP.tokenizer(text) if not (token.is_space or token.is_punct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 定义Field类\n",
    "声明数据预处理的pipeline\n",
    "\n",
    "**注意：这里的batch_size=True设置没起到作用???**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "unk_token = '<unk>'    # 数据集中有<unk>，所以该声明可以不使用\n",
    "pad_token = '<pad>'    # 文本数据是大段的，与翻译等其他数据集不同，所以不需要padding ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "# 将字符串小写，使用tokenizer函数处理数据\n",
    "TEXT = Field(sequential=True, lower=True, tokenize=tokenizer, use_vocab=True, batch_first=True,\n",
    "            init_token=sos_token, eos_token=eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join('../..', 'data','wikitext_tiny')     # 设置数据的根目录\n",
    "data_list = ['train', 'val', 'test']                          # 设置train,val,test路径\n",
    "train_txt, val_txt, test_txt = map(lambda x:os.path.join(data_root, 'wiki_'+x+'.tokens'), data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进过`LanguageModelingDataset`处理后，得到数据集为`{'text':[words,...]}`字典，只有一个'text'的keys，values是数据集的所有文本，在该方法处理后会自动的在每一行后面添加一个`<eos>`符号表示结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torchtext.datasets  import LanguageModelingDataset\n",
    "# 创建数据集\n",
    "train_dataset, val_dataset, test_dataset = LanguageModelingDataset.splits(   # 同时处理多个数据集\n",
    "    path = data_root,       \n",
    "    train = 'wiki_train.tokens', validation = 'wiki_val.tokens', test = 'wiki_test.tokens',\n",
    "    text_field = TEXT)   # 设置newline_eos=False每行不会补<eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homarus', 'gammarus', 'known', 'as', 'the', 'european', 'lobster']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0].__dict__['text'])      # 训练集共50396个单词\n",
    "train_dataset[0].__dict__['text'][5:12]     # 显示训练集中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用训练集建立字典(词汇表)\n",
    "使用GloVe的200维度预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 建立字典并使用预训练的词向量，词向量与程序在同一个路径下\n",
    "TEXT.build_vocab(train_dataset,  vectors=\"glove.6B.200d\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.字典的大小: 6421\n",
      "2.字典中的字符: ['<unk>', '<pad>', '<sos>', '<eos>', 'the', 'of', 'in']\n",
      "3.出现频率高的词: [('the', 3838), ('<unk>', 3354), ('of', 1514), ('in', 1445), ('and', 1421)]\n"
     ]
    }
   ],
   "source": [
    "print('1.字典的大小:', len(TEXT.vocab.itos))\n",
    "print('2.字典中的字符:', TEXT.vocab.itos[:7])                 # 显示词典中的前七个词\n",
    "print('3.出现频率高的词:', TEXT.vocab.freqs.most_common(5))   # 显示出现频率最高的词 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建迭代器\n",
    "BPTTIterator专门为语言建模构建的迭代器，它会生成一个timestep延迟的输入序列，属性text可以得到输入，target属性得到输出，结果类似如下：\n",
    "\n",
    "- 输入text：`<SOS> Life is short, I use python.`\n",
    "- 输出target：`Life is short, I use python. <EOS>`\n",
    "\n",
    "但使用BPTTIterator得到的结果与上述稍有不同，在<SOS>与<EOS>位置仍然是一个单词，而不是这种特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BPTTIterator, Iterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BPTTIterator.splits((train_dataset, val_dataset, test_dataset),\n",
    "    batch_size=batch_size, bptt_len=seq_len, device=-1, repeat=False)  # -1表示使用cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.数据维度: torch.Size([25, 32])\n",
      "2.输入: tensor([   3,  411,  198,    3,    3,  411,  198,  199,   13,    4,  932,  450,\n",
      "          62,  350,  450,   12,    9,  203,    5,    0,  450,   20,    4,  408,\n",
      "        1281])\n",
      "3.输出: tensor([ 411,  198,    3,    3,  411,  198,  199,   13,    4,  932,  450,   62,\n",
      "         350,  450,   12,    9,  203,    5,    0,  450,   20,    4,  408, 1281,\n",
      "         976])\n",
      "4.查看字典键值: dict_keys(['batch_size', 'dataset', 'train', 'fields', 'text', 'target'])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)                    # 生成的批数据维度顺序是 (seq,batch) \n",
    "a = next(it)\n",
    "print('1.数据维度:', a.text.shape)        # (seq_len, batch_size)\n",
    "print('2.输入:', a.text[:,0].squeeze())        # 输入比目标序列延迟一个step，但没有使用<SOS><EOS>标志???\n",
    "print('3.输出:', a.target[:,0].squeeze())  \n",
    "print('4.查看字典键值:', vars(a).keys())   # vars函数查看字典中的keys和values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.输入: of 60 cm 24 in and a mass of 6 kilograms 13 lb and bears a conspicuous pair of claws in life the lobsters are\n",
      "2.输出: 60 cm 24 in and a mass of 6 kilograms 13 lb and bears a conspicuous pair of claws in life the lobsters are blue\n"
     ]
    }
   ],
   "source": [
    "a = next(it)\n",
    "def idx2sent(seq_tensor):\n",
    "    \"\"\"\n",
    "    func：将seq_tensor转换为一个句子\n",
    "    seq_tensor：需要转换为句子的tensor，batch_size必须为1 \n",
    "    \"\"\"\n",
    "    seq_flatten = seq_tensor.flatten()      # 变为一维的向量\n",
    "    seq_list = [TEXT.vocab.itos[idx] for idx in seq_flatten]\n",
    "    seq_str = ' '.join(seq_list)\n",
    "    return seq_str\n",
    "text = idx2sent(a.text[:,0])\n",
    "target = idx2sent(a.target[:,0])\n",
    "print('1.输入:', text)\n",
    "print('2.输出:', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结**\n",
    "\n",
    "上述数据预处理的方式是按照普通的数据集处理流程来的，但torchtext中已经提供了几个常用的数据集，可以直接导入，(其实这些类中的实现方式与上述方法相同)方法如下：\n",
    "```python\n",
    "from torchtext.data import Field\n",
    "from torchtext.datasets import WikiText2\n",
    "# 声明数据预处理方式\n",
    "TEXT = Field(sequential=True, lower=True, tokenize=custom_tokenizer, use_vocab=True)\n",
    "# 创建数据集\n",
    "train, val, test = WikiText2.splits(TEXT)\n",
    "```\n",
    "语言建模类中提供了`WikiText2`,`WikiText103`,`PennTreebank`3种数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义语言模型\n",
    "需要注意的是lstm接收隐藏状态和细胞状态，并且是以(hidden_state, cell_state)的形式传入的，这里将两个状态写到一个`init_hidden`函数中，在模型中定义时默认是使用cpu，为了使用GPU则`model.to(device)`指定模型使用的设备，初始的状态同样也要指定设备，由于`init_hidden`函数返回的是`(hidden_state, cell_state)`元组，所以在外部指定设备时需要使用`GPU = lambda x:x.to(device)`和`hidden_0 = tuple(map(GPU, hidden_0))`将隐藏状态和细胞状态分别放到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,\n",
    "                 batch_size, n_layers=2, dropout=0.5):\n",
    "        \"\"\"\n",
    "        function: 语言模型的初始化\n",
    "        vocab_size：字典的大小\n",
    "        embed_size：词嵌入的维度大小\n",
    "        hidden_size：RNN中隐藏状态的维度\n",
    "        batch_size：批次数据的大小\n",
    "        n_layers：  RNN的层数，默认是1\n",
    "        dropout：词嵌入后进行的dropout正则化，默认为0.5\n",
    "        \"\"\"\n",
    "        super(LanguageModel, self).__init__()                 # 继承类的初始化\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size                          # 批次大小\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)     # 嵌入为embed_size维\n",
    "        # LSTM的dropout在多层中使用，除了输出层其他层都使用dropout\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=n_layers, dropout=dropout)\n",
    "        self.predictor = nn.Linear(hidden_size, vocab_size)   # 输出维度为词典大小\n",
    "        \n",
    "        self.init_weights()                                   # 初始化词嵌入的权重\n",
    "        # 输入的文本是分批次的连续语料，所以需要保持不同批次间的RNN隐藏状态\n",
    "        self.hidden_state = self.init_hidden()      # 保留上个批次的隐藏状态\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.predictor.weight.data.uniform_(-initrange, initrange)\n",
    "        self.predictor.bias.data.zero_()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        hidden_state = torch.zeros([self.n_layers, self.batch_size, self.hidden_size])\n",
    "        cell_state = torch.zeros_like(hidden_state)\n",
    "        # lstm接收参数是元组的形式，所以这里返回元组\n",
    "        return (hidden_state, cell_state)\n",
    "    \n",
    "    def reset_history(self):\n",
    "        # 将上个批次的状态数据送入下个批次使用，只有返回的是元组!!!\n",
    "        return tuple(state.data for state in self.hidden_state)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        x：输入的tensor (seq, batch)\n",
    "        hidden_state：lstm的起始隐藏状态 \n",
    "        \"\"\"\n",
    "        embedded = self.drop(self.embed(x))       # (seq, batch, embed_size)\n",
    "        # 注意这里的lstm将当前时刻输出状态会送入下一个时刻继续使用\n",
    "        lstm_out, self.hidden_state = self.lstm(embedded, hidden_state) # (seq,batch,hidden)\n",
    "        seq_len, batch = lstm_out.shape[:2]                     # 获取lstm输出的维度\n",
    "        lstm_flatten = lstm_out.view(-1, self.hidden_size)      # (seq*batch, hidden_size)\n",
    "        predictor = self.predictor(lstm_flatten)                # (seq*batch, vocab_size)\n",
    "        \n",
    "        return predictor.view(seq_len, batch, self.vocab_size)  # 恢复维度 (seq,batch,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6421])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试语言模型\n",
    "test_in = torch.randint(0,6421, [5,3], dtype=torch.long)\n",
    "test_in = test_in.to(device)\n",
    "test_model = LanguageModel(vocab_size=6421, embed_size=200, hidden_size=128,batch_size=3)\n",
    "test_model.to(device)\n",
    "hidden_0 = test_model.init_hidden()\n",
    "GPU = lambda x:x.to(device)\n",
    "hidden_0 = tuple(map(GPU, hidden_0))    # hidden_0返回是(hidden,cell)形式的元组，所以这样处理!!!\n",
    "out = languagemodel(test_in, hidden_0)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.字典大小和嵌入的维度大小: torch.Size([6421, 200])\n"
     ]
    }
   ],
   "source": [
    "embedded_matrix = TEXT.vocab.vectors     # 已载入的GloVe词嵌入权重\n",
    "print('1.字典大小和嵌入的维度大小:', embedded_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (embed): Embedding(6421, 200)\n",
       "  (lstm): LSTM(200, 200, num_layers=2, dropout=0.5)\n",
       "  (predictor): Linear(in_features=200, out_features=6421, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, embedded_size = embedded_matrix.shape\n",
    "# 定义语言模型\n",
    "languagemodel = LanguageModel(vocab_size, embedded_size, hidden_size=200, batch_size=batch_size)\n",
    "# 使用GloVe的词嵌入权重来初始化语言模型的词嵌入权重!!!\n",
    "languagemodel.embed.weight.data.copy_(embedded_matrix)\n",
    "languagemodel.to(device)          # 将模型放到GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数及优化方法\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(languagemodel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train Loss: 164.7618 Val Loss: 144.0621\n",
      "Epoch: 2   Train Loss: 162.5976 Val Loss: 142.2398\n",
      "Epoch: 3   Train Loss: 160.3848 Val Loss: 140.9112\n",
      "Epoch: 4   Train Loss: 158.3221 Val Loss: 139.9527\n",
      "Epoch: 5   Train Loss: 156.3918 Val Loss: 139.2746\n",
      "Epoch: 6   Train Loss: 154.5932 Val Loss: 138.5518\n",
      "Epoch: 7   Train Loss: 152.7538 Val Loss: 138.1737\n",
      "Epoch: 8   Train Loss: 151.1764 Val Loss: 137.7653\n",
      "Epoch: 9   Train Loss: 149.3675 Val Loss: 137.7146\n",
      "Epoch: 10  Train Loss: 147.7326 Val Loss: 137.4026\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, epochs=1):\n",
    "    \"\"\"\n",
    "    func：训练模型\n",
    "    train_data：训练数据集\n",
    "    val_data：验证集\n",
    "    epochs：训练epochs\n",
    "    \"\"\"\n",
    "    pltLoss = []\n",
    "    running_loss = 0\n",
    "    GPU = lambda x:x.to(device)                        # 优先使用GPU\n",
    "    model.to(device)\n",
    "    hidden_state = model.init_hidden()                 # 初始化隐藏状态!!!\n",
    "    hidden_state = tuple(map(GPU, hidden_state))\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()                                  # 训练模式\n",
    "        for data in train_loader:\n",
    "            x, y = data.text, data.target              # (seq,batch)\n",
    "            x, y = map(GPU, [x, y])\n",
    "            preds = model(x, hidden_state)\n",
    "            \n",
    "            preds_flatten = preds.view(-1, vocab_size)\n",
    "            y_flatten = y.view(-1)\n",
    "#             print(preds_flatten.shape, y_flatten.shape)\n",
    "            \n",
    "            loss = criterion(preds_flatten, y_flatten)\n",
    "            print_loss = loss.item()                   # 获取loss数据            \n",
    "            optimizer.zero_grad()                      # 更新参数\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += print_loss * x.shape[0]\n",
    "\n",
    "        hidden_state = model.reset_history()   # !!!将当前批次的隐藏状态数据放到下一个批次\n",
    "        epoch_loss = running_loss / len(train_loader)           # 计算epoch的损失\n",
    "        running_loss = 0                                        # 切记epoch完成后清零\n",
    "        pltLoss.append(epoch_loss)\n",
    "        \n",
    "        # 计算验证集上的误差\n",
    "        val_loss = 0\n",
    "        model.eval()                                            # 模型进入测试模型\n",
    "        with torch.no_grad():\n",
    "            for data_val in val_loader:\n",
    "                x_val, y_val = data_val.text, data_val.target   # (seq,batch)\n",
    "                x_val, y_val = map(GPU, [x_val, y_val])         # 数据放到GPU上\n",
    "                preds = model(x_val, hidden_state)  \n",
    "                preds_flatten = preds.view(-1, vocab_size)\n",
    "                y_flatten = y_val.view(-1)\n",
    "                loss = criterion(preds_flatten, y_flatten)\n",
    "                val_loss += loss.item() * x_val.shape[0]\n",
    "            epoch_val_loss = val_loss / len(val_loader)\n",
    "        print('Epoch: {:<3} Train Loss: {:<8.4f} Val Loss: {:<7.4f}'.format(\n",
    "            epoch, epoch_loss, epoch_val_loss))\n",
    "    return model, pltLoss          \n",
    "train_model, pltloss = train(languagemodel, train_iter, val_iter, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a4f82deba8>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a496ca7278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.plot(pltloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
